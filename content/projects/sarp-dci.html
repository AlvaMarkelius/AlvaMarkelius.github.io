---
title: "SARP-DCI"
---


<p>
  This project is part of the DICE-lab at the Institution of Applied IT at the University of Gothenburg and looks to investigate the effects of a social robot partner, which adapts its behaviours and interactions based on a human partner's affective and emotional states, on the long-term viability (acceptance, adherence, motivation, efficacy) of digitised cognitive training therapy. In combination with interactions with a social robot and a cognitive training task, this project seeks to find out: (1) the optimal combination of multimodal signals that can best capture a user's affective/emotional states, particularly when interacting with these tools, (2) the effects of real-time adaptation of these technologies (robot interactions, characteristics of training tasks) based on these affective states on various behavioural and performance measures and (3) the viability of this approach in pre-clinical populations with Mild Cognitive Impairments.
</p>

<p>
  This project have been generously supported by funding from the Swedish Foundation for International Cooperation in Research and Higher Education (STINT) as a collaboration with Koç University in Turkyie.
</p>

<div style="margin: 20px 0;">
  <img src="/projects/8.jpg" alt="example image" style="width: 100%; max-width: 100%; height: auto; border-radius: 5px;">
</div>

<div style="font-size: 14px; color: inherit; margin-top: 10px; text-align: center;">
  <span>
    The project includes a setup with the humanoid robot Furhat assisting participants who perform a memory training task on a tablet or laptop.
  </span>
</div>

<p>
  We conducted the first study in spring 2022, involving 78 participants playing a gamified version of an existing visuospatial working memory task administered according to a differential outcomes training (DOT) approach. The interactive setup included Furhat as a socially assistive robot (SAR) providing performance feedback and task instructions. The results showed an overall significant differential outcomes effect (DOE) for memory performance accuracy, which is the first time, a DOE was observed in a gamified context using a SAR. The results from the affective ratings revealed that participants accompanied by the robot reported lower levels of negative affect and feeling more in control. Additionally, we conducted exploratory analyses of eye tracking data to investigate eye movement strategies during the memory encoding phase of the task, which gave initial insight into the different memorisation strategies and encoding during the task.
</p>

<p>
  A second study was carried out at Koç University in Turkyie and involved 60 participants who took part of a similar study although this time investigating different types of feedback. They carried out the same gamified memory training task with a DOT protocol, although this time receiving either affective, performance-based or mixed feedback from the robot during the task. The setup also included an algorithm allowing the robot to alter the feedback depending on the perceived affective state of the participant and the performance on the memory task.
</p>

<h2 style="margin-top: 50px; margin-bottom: 20px; color: inherit; border-bottom: 2px solid #666; padding-bottom: 8px;">Related Publications</h2>

<ul style="list-style: none; padding-left: 0;">

  <li>
    Ravandi, B. S., Khan, I., <strong>Markelius, A.</strong>, Bergström, M., Gander, P., Erzin, E., & Lowe, R. (2025).  
    Exploring task and social engagement in companion social robots: a comparative analysis of feedback types.  
    <em>Advanced Robotics</em> <a href="https://doi.org/10.1080/01691864.2025.2526668" target="_blank">[DOI]</a>
  </li>

  <li>
    <strong>Markelius, A.</strong>, Sjöberg, S., Bergström, M., Ravandi, B. S., Vivas, A. B., Khan, I., & Lowe, R. (2023).  
    Differential Outcomes Training of Visuospatial Memory: A Gamified Approach Using a Socially Assistive Robot.  
    <em>International Journal of Social Robotics</em> <a href="https://doi.org/10.1007/s12369-023-01083-0" target="_blank">[DOI]</a>
  </li> 

</ul>